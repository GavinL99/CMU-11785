{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import Levenshtein\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class ModelManager:\n",
    "    def __init__(self, model, train_loader, eval_loader, load_model=None):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.eval_loader = eval_loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.val_edit_dist = []\n",
    "        self.epochs = 0\n",
    "        self.n_vocab = len(VOCAB_DICT)\n",
    "        self.print_interval = 10\n",
    "\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE,\n",
    "                                    weight_decay=WEIGHT_DECAY)\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=3,\n",
    "                                                   gamma=0.8)\n",
    "#         self.train_criterion = nn.CrossEntropyLoss(reduction=\"sum\", ignore_index=-1)\n",
    "        self.train_criterion = nn.CrossEntropyLoss(ignore_index=-1)\n",
    "\n",
    "\n",
    "        if load_model:\n",
    "            model_path = \"./Models/model_epoch_{}.pth\".format(load_model[\"epoch_num\"])\n",
    "            if torch.cuda.is_available():\n",
    "                state = torch.load(model_path)\n",
    "            else:\n",
    "                state = torch.load(model_path, map_location='cpu')\n",
    "            model_state = state[\"model\"]\n",
    "            opt_state = state[\"optimizer\"]\n",
    "            self.epochs = state[\"epoch\"]\n",
    "            self.model.load_state_dict(model_state)\n",
    "            self.model.to(DEVICE)\n",
    "            self.scheduler.load_state_dict(state[\"scheduler\"])\n",
    "            self.optimizer.load_state_dict(opt_state)\n",
    "            # move optimizer params to cuda\n",
    "            if torch.cuda.is_available():\n",
    "                for state in self.optimizer.state.values():\n",
    "                    for k, v in state.items():\n",
    "                        if isinstance(v, torch.Tensor):\n",
    "                            state[k] = v.cuda()\n",
    "        else:\n",
    "            self.model.init_weights()\n",
    "            self.model.to(DEVICE)\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "        n_batch = self.train_loader.batch_size\n",
    "        self.scheduler.step()\n",
    "\n",
    "        for batch_num, (inputs, targets, input_lens, target_lens)                 in enumerate(self.train_loader):\n",
    "            self.optimizer.zero_grad()\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            targets = targets.to(DEVICE)\n",
    "            # (B, L, N_vocab), (B, L)\n",
    "            decode_output, tgt_expand, attention_score = self.model(inputs, input_lens, targets, target_lens)\n",
    "            decode_output = decode_output.view(-1, self.n_vocab)\n",
    "            tgt_flatten = tgt_expand.flatten()\n",
    "            loss = self.train_criterion(decode_output, tgt_flatten)\n",
    "#             perplexity = np.exp((loss.cpu().detach() / sum(target_lens)).numpy())\n",
    "            \n",
    "            perplexity = np.exp((loss.cpu().detach()).numpy())\n",
    "#             loss /= n_batch\n",
    "\n",
    "            loss.backward()\n",
    "            # clipping\n",
    "            torch.nn.utils.clip_grad_value_(self.model.parameters(), 5.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "            del inputs\n",
    "            del targets\n",
    "            if batch_num % self.print_interval == 0:\n",
    "                print(\"Batch: {} / Total: {} Loss: {} Perplexity: {}\".format(batch_num, str(\n",
    "                    len(self.train_loader)), loss, perplexity))\n",
    "                if batch_num > 0:\n",
    "                    ModelManager.save_attention_plot(attention_score[:, 0, :].cpu().detach().numpy(), self.epochs, batch_num)\n",
    "\n",
    "                    ModelManager.plot_grad_flow(self.model.named_parameters(), self.epochs, batch_num)\n",
    "\n",
    "        epoch_loss = epoch_loss / (batch_num + 1)\n",
    "        self.train_losses.append(epoch_loss)\n",
    "        self.epochs += 1\n",
    "\n",
    "    def evaluate(self, beam_search=False):\n",
    "        # greedy search\n",
    "        self.model.eval()\n",
    "        model.to(DEVICE)\n",
    "        assert self.eval_loader.batch_size == 1\n",
    "        perplexity_total = 0.0\n",
    "        edit_dist_total = 0.0\n",
    "\n",
    "        if not beam_search:\n",
    "            for batch_num, (inputs, targets, input_lens, target_lens)                     in enumerate(self.eval_loader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                decode_output, perplexity = self.model.inference(inputs, input_lens, False)\n",
    "                translated_output = UtteranceDataset.decode_symbols(VOCAB_DICT,\n",
    "                                                                    NUM_TO_CHAR,\n",
    "                                                                    decode_output)\n",
    "                translated_tgt = UtteranceDataset.decode_symbols(VOCAB_DICT,\n",
    "                                                                 NUM_TO_CHAR,\n",
    "                                                                 targets[0])\n",
    "                edit_dist_total += Levenshtein.distance(translated_output,\n",
    "                                                        translated_tgt)\n",
    "                perplexity_total += perplexity\n",
    "\n",
    "                if batch_num % 200 == 0:\n",
    "                    print(\"Target Transcript: {}\\nOutput: {}\".format(\n",
    "                        translated_tgt, translated_output))\n",
    "\n",
    "            perplexity_total /= len(self.eval_loader)\n",
    "            edit_dist_total /= len(self.eval_loader)\n",
    "            print('[VAL]  Epoch [%d/%d]   Perplexity: %.4f  Edit_Dist: %.4f'\n",
    "                  % (self.epochs, NUM_EPOCHS, perplexity_total, edit_dist_total))\n",
    "            return edit_dist_total, perplexity_total\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def predict(self, test_loader, beam_search=False):\n",
    "        # greedy search\n",
    "        print(\"Prediction!\")\n",
    "        self.model.eval()\n",
    "        model.to(DEVICE)\n",
    "        assert self.eval_loader.batch_size == 1\n",
    "        output = []\n",
    "\n",
    "        if not beam_search:\n",
    "            for batch_num, (inputs, input_lens)                     in enumerate(test_loader):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                decode_output, _ = self.model.inference(inputs, input_lens, False)\n",
    "                translated_output = UtteranceDataset.decode_symbols(VOCAB_DICT,\n",
    "                                                                    NUM_TO_CHAR,\n",
    "                                                                    decode_output)\n",
    "                output.append(translated_output)\n",
    "                if batch_num % 200 == 0:\n",
    "                    print(\"Predicted Transcript: {}\".format(translated_output))\n",
    "            return output\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def save(self):\n",
    "        state = {\n",
    "            \"epoch\": self.epochs,\n",
    "            \"model\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "            \"scheduler\": self.scheduler.state_dict()\n",
    "        }\n",
    "        torch.save(state, \"./Models/model_epoch_{}.pth\".format(self.epochs))\n",
    "\n",
    "    def set_teacher_force_rate(self, new_p):\n",
    "        self.model.decoder.teacher_force_p = new_p\n",
    "        print(\"New Teacher Forcing Rate: {}\"\n",
    "              .format(self.model.decoder.teacher_force_p))\n",
    "\n",
    "    def apply_train_policy(self):\n",
    "        \"\"\"\n",
    "        Set teacher force rate, gumbel noise\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self.epochs <= 5:\n",
    "            self.set_teacher_force_rate(1.0 + 0.1)\n",
    "#         elif self.epochs <= 15:\n",
    "#             self.set_teacher_force_rate(0.9)\n",
    "        else:\n",
    "            self.set_teacher_force_rate(0.9)\n",
    "\n",
    "#         if self.epochs <= 15:\n",
    "#             self.model.decoder.gumble = False\n",
    "#         else:\n",
    "#             self.model.decoder.gumble = True\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def save_attention_plot(cls, attention_weights, epoch, batch_num):\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.imshow(attention_weights)\n",
    "        fig.savefig(\"../Attention_Plots/epoch%d-%d.png\" % (epoch, batch_num))\n",
    "        plt.close()\n",
    "\n",
    "    @classmethod\n",
    "    def plot_grad_flow(cls, named_parameters, epoch, batch_num):\n",
    "        # pass\n",
    "        ave_grads = []\n",
    "        layers = []\n",
    "        for n, p in named_parameters:\n",
    "            if p.requires_grad and \"bias\" not in n:\n",
    "                layers.append(n)\n",
    "                ave_grads.append(p.grad.abs().mean())\n",
    "\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        plt.xticks(rotation=30)\n",
    "        plt.plot(ave_grads, alpha=0.3, color=\"b\")\n",
    "        plt.hlines(0, 0, len(ave_grads) + 1, linewidth=1, color=\"k\")\n",
    "        plt.xticks(range(0, len(ave_grads), 1), layers, rotation=\"vertical\")\n",
    "        plt.xlim(xmin=0, xmax=len(ave_grads))\n",
    "        plt.xlabel(\"Layers\")\n",
    "        plt.ylabel(\"average gradient\")\n",
    "        plt.title(\"Gradient flow\")\n",
    "        plt.grid(True)\n",
    "        fig.savefig(\"../Gradient_Plots/epoch%d-%d.png\" % (epoch, batch_num), bbox_inches='tight')\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "BATCH_SZ = 8\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 30\n",
    "MAX_DECODE_LEN = 200\n",
    "N_ENCODER = 256\n",
    "N_DECODER = 2 * N_ENCODER\n",
    "N_ATTENTION = 128\n",
    "N_EMBED = 256\n",
    "TEACHER_FORCE_P = 0.9\n",
    "ATTENTION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#\n",
    "# os.chdir(\"LAS\")\n",
    "train_np = np.load(\"../data/train.npy\", encoding=\"bytes\")\n",
    "train_tgt = np.load(\"../data/train_transcripts.npy\", encoding=\"bytes\")\n",
    "eval_np = np.load(\"../data/dev.npy\", encoding=\"bytes\")\n",
    "eval_tgt = np.load(\"../data/dev_transcripts.npy\", encoding=\"bytes\")\n",
    "\n",
    "data = UtteranceDataset(get_char_set(train_tgt), train_np, train_tgt)\n",
    "VOCAB_DICT, NUM_TO_CHAR = data.get_dict()\n",
    "data_eval = UtteranceDataset(VOCAB_DICT, eval_np, eval_tgt)\n",
    "collate_f = lambda x: collate_utterance(x, VOCAB_DICT[\"ignore\"])\n",
    "train_loader = DataLoader(data, shuffle=True, batch_size=BATCH_SZ,\n",
    "                          collate_fn=collate_f, drop_last=True)\n",
    "eval_loader = DataLoader(data_eval, shuffle=False, batch_size=1,\n",
    "                         collate_fn=collate_f, drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention is On\n"
     ]
    }
   ],
   "source": [
    "data = UtteranceDataset(get_char_set(train_tgt), train_np, train_tgt)\n",
    "VOCAB_DICT, NUM_TO_CHAR = data.get_dict()\n",
    "data_eval = UtteranceDataset(VOCAB_DICT, eval_np, eval_tgt)\n",
    "\n",
    "collate_f = lambda x: collate_utterance(x, VOCAB_DICT[\"ignore\"])\n",
    "train_loader = DataLoader(data, shuffle=True, batch_size=BATCH_SZ,\n",
    "                          collate_fn=collate_f, drop_last=True)\n",
    "eval_loader = DataLoader(data_eval, shuffle=False, batch_size=1,\n",
    "                         collate_fn=collate_f, drop_last=True)\n",
    "\n",
    "encoder_params = {\n",
    "    \"n_frame\": 40,\n",
    "    \"n_hid\": N_ENCODER,\n",
    "    \"n_attention\": N_ATTENTION\n",
    "}\n",
    "\n",
    "if ATTENTION:\n",
    "    attention_params = {\n",
    "        \"n_encoder\": N_ENCODER * 2,\n",
    "        \"n_decoder\": N_DECODER,\n",
    "        \"n_attention\": N_ATTENTION,\n",
    "        \"DEVICE\": DEVICE\n",
    "    }\n",
    "else:\n",
    "    attention_params = None\n",
    "\n",
    "decoder_params = {\n",
    "    \"n_vocab\": len(VOCAB_DICT),\n",
    "    \"vocab\": VOCAB_DICT,\n",
    "    \"n_hid\": N_DECODER,\n",
    "    \"n_embed\": N_EMBED,\n",
    "    \"attention_param\": attention_params,\n",
    "    \"max_decode_len\": MAX_DECODE_LEN,\n",
    "    \"eos_idx\": VOCAB_DICT[\"<eos>\"],\n",
    "    \"sos_idx\": VOCAB_DICT[\"<sos>\"],\n",
    "    \"DEVICE\": DEVICE,\n",
    "    \"WEIGHT_TYING\": True,\n",
    "    \"BEAM_SIZE\": None,\n",
    "    \"GUMBLE\": False\n",
    "}\n",
    "\n",
    "model = LAS(encoder_params, decoder_params)\n",
    "manager = ModelManager(model, train_loader, eval_loader, {\"epoch_num\": 26})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Epoch: 34\n",
      "New Teacher Forcing Rate: 0.9\n",
      "Batch: 0 / Total: 3090 Loss: 0.08281967788934708 Perplexity: 1.086345911026001\n",
      "Batch: 10 / Total: 3090 Loss: 0.07965816557407379 Perplexity: 1.0829168558120728\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py:3215: MatplotlibDeprecationWarning: \n",
      "The `xmin` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `left` instead.\n",
      "  alternative='`left`', obj_type='argument')\n",
      "/opt/anaconda3/lib/python3.7/site-packages/matplotlib/axes/_base.py:3221: MatplotlibDeprecationWarning: \n",
      "The `xmax` argument was deprecated in Matplotlib 3.0 and will be removed in 3.2. Use `right` instead.\n",
      "  alternative='`right`', obj_type='argument')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 20 / Total: 3090 Loss: 0.05578459054231644 Perplexity: 1.0573699474334717\n",
      "Batch: 30 / Total: 3090 Loss: 0.07087723165750504 Perplexity: 1.0734493732452393\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-a8fdbdb6fe3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Start Epoch: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_train_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mentropy_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medit_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmanager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-dc4ec1d782c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;31m#             loss /= n_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m             \u001b[0;31m# clipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_value_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_dist = None\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(\"Start Epoch: {}\".format(manager.epochs))\n",
    "    manager.apply_train_policy()\n",
    "    manager.train()\n",
    "    entropy_loss, edit_dist = manager.evaluate(False)\n",
    "        \n",
    "#    if best_dist is None or edit_dist < best_dist:\n",
    "    best_dist = edit_dist\n",
    "    print(\n",
    "            \"Saving model, predictions and generated output for epoch \" + str(\n",
    "                epoch) + \" with Edit Dist: \" + str(best_dist))\n",
    "    manager.save()\n",
    "\n",
    "\n",
    "# In[55]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction!\n",
      "Predicted Transcript: his done vary welf for stock holder\n",
      "Predicted Transcript: that's more are law swate the pluce chip economists said spect period\n",
      "Predicted Transcript: the plan was discovered after decoding daugata captured at last week's arrester twenty rebel eaters\n"
     ]
    }
   ],
   "source": [
    "collate_f = lambda x: collate_utterance(x, VOCAB_DICT[\"ignore\"])\n",
    "\n",
    "test_np = np.load(\"../data/test.npy\", encoding=\"bytes\")\n",
    "data_test = UtteranceDataset(VOCAB_DICT, test_np, None)\n",
    "test_loader = DataLoader(data_test, shuffle=False, batch_size=1,\n",
    "                         collate_fn=collate_f, drop_last=True)\n",
    "pred_tscp = manager.predict(test_loader, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Id                                          Predicted\n",
      "0   0                HIS DONE VARY WELF FOR STOCK HOLDER\n",
      "1   1  THE COMPANY ALSO DON'T AID FIFTY THOUSAND DOLL...\n",
      "2   2  THE GOLD CAR WHICH INCRUDES A FEW ADDITIONAL S...\n",
      "3   3  NO FIRM PLANT HAS BEEN DEVISED BUT IT IS UNDER...\n",
      "4   4  THEY NOTED THAT CONSUMER IS MAY HAVE STEPPED U...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "out_df = pd.DataFrame()\n",
    "out_df['Id'] = np.arange(0, len(pred_tscp))\n",
    "out_df['Predicted'] = [x.upper() for x in pred_tscp]\n",
    "print(out_df.head())\n",
    "out_df.to_csv('submission_guanfu.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = torch.Tensor([[1, 2, 3], [4,5,6]])\n",
    "v1 = torch.Tensor([[1, 2, 3], [4,5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([v, v1], dim=0).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
